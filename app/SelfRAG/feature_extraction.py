# feature_extraction.py
# -------------------------------------------------------------
# B∆∞·ªõc 2: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (embeddings) t·ª´ h√¨nh ·∫£nh v√† caption b·∫±ng CLIP
# -------------------------------------------------------------

from PIL import Image
from sentence_transformers import SentenceTransformer
import torch
import os
import pickle
from tqdm import tqdm
import numpy as np

# -------------------------------------------------------------
# ‚öôÔ∏è Kh·ªüi t·∫°o model CLIP
# -------------------------------------------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
model = SentenceTransformer('clip-ViT-B-32', device=device)
print(f"üî• Model ƒëang ch·∫°y tr√™n: {device.upper()}")

# -------------------------------------------------------------
# üß† H√†m ch√≠nh
# -------------------------------------------------------------
def extract_features(dataset_path='app/SelfRAG/dataset', 
                     output_path='app/SelfRAG/clip_features.pkl'):
    """
    1Ô∏è‚É£ Duy·ªát t·∫•t c·∫£ ·∫£nh trong dataset/
    2Ô∏è‚É£ ƒê·ªçc caption t∆∞∆°ng ·ª©ng
    3Ô∏è‚É£ D√πng CLIP encode c·∫£ ·∫£nh v√† caption
    4Ô∏è‚É£ L∆∞u vector embeddings v√†o file .pkl
    """
    data = []

    for folder in tqdm(os.listdir(dataset_path), desc="Extracting features"):
        folder_path = os.path.join(dataset_path, folder)
        if not os.path.isdir(folder_path):
            continue

        image_path = os.path.join(folder_path, 'image.jpg')
        caption_path = os.path.join(folder_path, 'caption.txt')

        if not os.path.exists(image_path) or not os.path.exists(caption_path):
            print(f"[‚ö†Ô∏è] Thi·∫øu file trong {folder_path}, b·ªè qua.")
            continue

        # ƒê·ªçc caption
        with open(caption_path, 'r', encoding='utf-8') as f:
            caption = f.read().strip()

        # M·ªü ·∫£nh (chuy·ªÉn v·ªÅ RGB ƒë·ªÉ tr√°nh l·ªói)
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"[‚ùå] Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c ·∫£nh {image_path}: {e}")
            continue

        # Encode b·∫±ng CLIP
        image_emb = model.encode(image, convert_to_tensor=True)
        text_emb = model.encode(caption, convert_to_tensor=True)

        # L∆∞u vector
        data.append({
            "id": folder,
            "caption": caption,
            "image_embedding": image_emb.cpu().numpy(),
            "text_embedding": text_emb.cpu().numpy()
        })

    # Ghi file
    with open(output_path, 'wb') as f:
        pickle.dump(data, f)

    print(f"\n‚úÖ ƒê√£ l∆∞u to√†n b·ªô ƒë·∫∑c tr∆∞ng v√†o {output_path}")

# -------------------------------------------------------------
# üöÄ Entry point
# -------------------------------------------------------------
if __name__ == "__main__":
    extract_features()
